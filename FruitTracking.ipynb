{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed998c8f-ac0d-4033-879a-5e34ad7e1c92",
   "metadata": {
    "id": "ed998c8f-ac0d-4033-879a-5e34ad7e1c92"
   },
   "source": [
    "# In case of using colab\n",
    "Remember to connect to your drive and upload the dataset if you want to repeat several times  this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e2e4fe-7062-4d11-b1a5-5975fcdd3b29",
   "metadata": {
    "id": "d9e2e4fe-7062-4d11-b1a5-5975fcdd3b29"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee2d9c8-3def-41c1-9214-dcb3c3cb6962",
   "metadata": {
    "id": "bee2d9c8-3def-41c1-9214-dcb3c3cb6962"
   },
   "source": [
    "# Download the dataset\n",
    "\n",
    "In this occasion we are going to use different video footage of cherry trees to test a tracking algorithm.\n",
    "\n",
    "\n",
    "[videos-cherry](https://drive.google.com/drive/folders/1H0jr8uhQLkxs_wNjQqUiews_CLIFUdMV?usp=sharing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4afe1b-3495-4ef1-a699-ad18e3fbee4f",
   "metadata": {
    "id": "2b4afe1b-3495-4ef1-a699-ad18e3fbee4f"
   },
   "source": [
    "# Install repositories\n",
    "The repository for this hands on is:\n",
    "\n",
    "```code\n",
    "git clone https://github.com/LuisCossioUOH/Hands-On-Lacoro2025-Fruit-Tracking\n",
    "```\n",
    "\n",
    "You can either clone the reposritory or upload the tracking_utils.py file:\n",
    "\n",
    "And install ultralytics\n",
    "```code\n",
    "pip install ultralytics==8.3.94\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5f2fed-afbd-417d-a3a8-bde7707a1baf",
   "metadata": {
    "id": "cf5f2fed-afbd-417d-a3a8-bde7707a1baf"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/LuisCossioUOH/Hands-On-Lacoro2025-Fruit-Tracking\n",
    "# !mv Hands-On-Lacoro2025-Fruit-Tracking/tracking_utils.py  ./tracking_utils.py\n",
    "!pip install ultralytics==8.3.94"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaad6ce-a31d-4453-a94e-3894db83f928",
   "metadata": {
    "id": "bfaad6ce-a31d-4453-a94e-3894db83f928"
   },
   "source": [
    "## Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b773d3f-97c9-487a-a945-8770d6430b18",
   "metadata": {
    "id": "7b773d3f-97c9-487a-a945-8770d6430b18"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import yaml\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from matplotlib.patches import Rectangle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66775638-fc4d-4ac0-a615-1c781eb8fd50",
   "metadata": {
    "id": "66775638-fc4d-4ac0-a615-1c781eb8fd50"
   },
   "source": [
    "# Track dataset Dummy\n",
    "1. Test the tracking dataset and check the difference between tracks and detections.\n",
    "2. Draw the detections in the image.\n",
    "3. Draw the tracks of each object with a diferent color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950f87e6-15ae-4dc0-b11b-89925d6dba76",
   "metadata": {
    "id": "950f87e6-15ae-4dc0-b11b-89925d6dba76"
   },
   "outputs": [],
   "source": [
    "from tracking_utils import TrackingEllipseGenerator\n",
    "\n",
    "dataset = TrackingEllipseGenerator(300,n_tracks=50,image_shape=[300,300])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36266c5-e828-40bd-8ec9-c42abd69b95d",
   "metadata": {
    "id": "d36266c5-e828-40bd-8ec9-c42abd69b95d"
   },
   "outputs": [],
   "source": [
    "frame_id = 10\n",
    "tracks,img = dataset.get_frame(frame_id)\n",
    "plt.imshow(img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707ffd78-93f7-402e-8ed9-d404b247705f",
   "metadata": {
    "id": "707ffd78-93f7-402e-8ed9-d404b247705f"
   },
   "outputs": [],
   "source": [
    "# frame_id = 100\n",
    "frame_id += 1\n",
    "tracks, image, dets = dataset.get_frame_and_detections(frame_id)\n",
    "\n",
    "color = (0,255,0)\n",
    "thickness = 2\n",
    "print(\"dets: \",dets)\n",
    "point1 = dets[0,:2].astype(np.int32)\n",
    "point2 = dets[0,2:4].astype(np.int32)\n",
    "image = cv2.rectangle(image, point1, point2, color=color, thickness=thickness)\n",
    "\n",
    "#### CODE HERE ####\n",
    "\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979d8bbb-5ef6-41bd-9e46-71e02920cb3b",
   "metadata": {
    "id": "979d8bbb-5ef6-41bd-9e46-71e02920cb3b"
   },
   "source": [
    "# Tracking a single object\n",
    "We are going to see if we can can build a tracker for a single object.\n",
    "1. Analize multiple frames and determine main challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c8f0eb-0f8d-4ec0-9ffa-dd6313acbaf0",
   "metadata": {
    "id": "16c8f0eb-0f8d-4ec0-9ffa-dd6313acbaf0"
   },
   "outputs": [],
   "source": [
    "dataset = TrackingEllipseGenerator(300,n_tracks=1,image_shape=[300,300],duration_samples=[299,300],\n",
    "                                   false_negative_rate=0,false_positive_rate=0.5)\n",
    "\n",
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfec725c-5a68-40ce-874b-3dba4e48a925",
   "metadata": {
    "id": "cfec725c-5a68-40ce-874b-3dba4e48a925"
   },
   "outputs": [],
   "source": [
    "idx += 1\n",
    "tracks,img,dets = dataset.get_frame_and_detections(idx,draw_detection=True)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f5c57f-4307-4562-9b4c-7c5a68064b9a",
   "metadata": {
    "id": "92f5c57f-4307-4562-9b4c-7c5a68064b9a"
   },
   "source": [
    "# Kalman tracking\n",
    "1. Define a function calculate the L2 distance between a vector of size [4] and a matrix of size [n,4]. And produce n distances.\n",
    "2. Modified the Tracker class to have a find_observation method. This recieves N new detections and return the closest to the currently tracked object. If there are no close detections then return and empty array.\n",
    "3. Modified to code to have an input \"u\" diferent than 0. The input \"u\" represent the acceleration.\n",
    "\n",
    "4. Modified the code to use the prior as the final track and increase the variance by 50% in the covariance matrix if there are no detections.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddc9ba7-7bb2-40c8-8fb7-9bf8ffc2091d",
   "metadata": {
    "id": "1ddc9ba7-7bb2-40c8-8fb7-9bf8ffc2091d"
   },
   "outputs": [],
   "source": [
    "class KalmanFilter:\n",
    "    def __init__(self, A: np.ndarray, B: np.ndarray, C: np.ndarray, D: np.ndarray, cov_model_diag=1.0,\n",
    "                 cov_obs_diag=1.0):\n",
    "        self.A = A\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.D = D\n",
    "        self.Q = np.eye(len(A)) * np.array([cov_model_diag])\n",
    "        self.R = np.eye(len(C)) * np.array([cov_obs_diag])\n",
    "\n",
    "    def prior(self, x, u):\n",
    "        return self.A @ x + self.B @ u\n",
    "\n",
    "    def observation(self, x, u):\n",
    "        return self.C @ x + self.D @ u\n",
    "\n",
    "    def posterior(self, x, prior_x, u, observation, P):\n",
    "        z_prior = self.observation(x, u)  # estimation\n",
    "        prior_P = self.A @ P @ self.A + self.Q\n",
    "        W = prior_P @ self.C.transpose() @ np.linalg.inv(self.C @ prior_P @ self.C.transpose() + self.R)  # Kalman gain\n",
    "        post_x = prior_x + W @ (observation - z_prior)  # posterior\n",
    "        post_P = (np.eye(len(P)) - W @ self.C) @ prior_P  #\n",
    "        return post_x, post_P\n",
    "\n",
    "def get_kalman_filter(variance_model= 20,variance_observation=40):\n",
    "    A = np.array([[1, 0, 0, 0, 1, 0, 0, 0],\n",
    "                  [0, 1, 0, 0, 0, 1, 0, 0],\n",
    "                  [0, 0, 1, 0, 0, 0, 1, 0],\n",
    "                  [0, 0, 0, 1, 0, 0, 0, 1],\n",
    "                  [0, 0, 0, 0, 1, 0, 0, 0],\n",
    "                  [0, 0, 0, 0, 0, 1, 0, 0],\n",
    "                  [0, 0, 0, 0, 0, 0, 1, 0],\n",
    "                  [0, 0, 0, 0, 0, 0, 0, 1]])\n",
    "\n",
    "    B = np.array([[0, 0, 0, 0],\n",
    "                  [0, 0, 0, 0],\n",
    "                  [0, 0, 0, 0],\n",
    "                  [0, 0, 0, 0],\n",
    "                  [1, 0, 0, 0],\n",
    "                  [0, 1, 0, 0],\n",
    "                  [0, 0, 1, 0],\n",
    "                  [0, 0, 0, 1]])\n",
    "\n",
    "    C = np.array([[1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                  [0, 1, 0, 0, 0, 0, 0, 0],\n",
    "                  [0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                  [0, 0, 0, 1, 0, 0, 0, 0],\n",
    "                  [0, 0, 0, 0, 1, 0, 0, 0],\n",
    "                  [0, 0, 0, 0, 0, 1, 0, 0],\n",
    "                  [0, 0, 0, 0, 0, 0, 1, 0],\n",
    "                  [0, 0, 0, 0, 0, 0, 0, 1]])\n",
    "\n",
    "    D = np.array([[0, 0, 0, 0],\n",
    "                  [0, 0, 0, 0],\n",
    "                  [0, 0, 0, 0],\n",
    "                  [0, 0, 0, 0],\n",
    "                  [0, 0, 0, 0],\n",
    "                  [0, 0, 0, 0],\n",
    "                  [0, 0, 0, 0],\n",
    "                  [0, 0, 0, 0]])\n",
    "\n",
    "    return KalmanFilter(A, B, C, D,variance_model, variance_observation)\n",
    "\n",
    "\n",
    "def format_detections(detections: np.ndarray, speed=0):\n",
    "    \"\"\"\n",
    "    Function to format detections into a position format\n",
    "    :param detections: detection array to be formatted into a matrix of columns [x1,y1,x2,y2,x'1,y'1,x'2,y'2]. Initial\n",
    "    speed is zero.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    detection = np.zeros([len(detections), 8])\n",
    "    detection[:, :4] = detections[:, :4]\n",
    "    detection[:, 4:] += speed\n",
    "    return detection\n",
    "\n",
    "kalman = get_kalman_filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdaa392-a885-4bdf-8142-76361eceb9d0",
   "metadata": {
    "id": "3bdaa392-a885-4bdf-8142-76361eceb9d0"
   },
   "outputs": [],
   "source": [
    "def calculate_iou_distance(target: np.ndarray, matrix2: np.ndarray):\n",
    "    \"\"\"\n",
    "    Calculate IoU correlation between locations in matrices\n",
    "    :param target: Position matrix array as [x1,y1,x2,y2] columns\n",
    "    :param matrix2: Position matrix array as [x1,y1,x2,y2] columns\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    target = np.reshape(target,[-1,4])\n",
    "    inter = np.maximum(np.minimum(target[:, None, 2], matrix2[:, 2]) - np.maximum(target[:, None, 0], matrix2[:, 0]),\n",
    "                       0)\n",
    "    inter *= np.maximum(np.minimum(target[:, None, 3], matrix2[:, 3]) - np.maximum(target[:, None, 1], matrix2[:, 1]),\n",
    "                        0)\n",
    "    eps = 0.000001\n",
    "\n",
    "    # Union Area\n",
    "    w1, h1 = target[:, 2] - target[:, 0], target[:, 3] - target[:, 1] + eps\n",
    "    w2, h2 = matrix2[:, 2] - matrix2[:, 0], matrix2[:, 3] - matrix2[:, 1] + eps\n",
    "    union = ((w1 * h1)[:, None] + w2 * h2) - inter + eps\n",
    "\n",
    "    iou = inter / union\n",
    "    return iou\n",
    "\n",
    "def calculate_distance(target: np.ndarray, matrix2: np.ndarray):\n",
    "    \"\"\"\n",
    "    Calculate the distance between the target vector and some detection matrix\n",
    "    :param target: Position matrix array as [x1,y1,x2,y2] columns\n",
    "    :param matrix2: Position matrix array as [x1,y1,x2,y2] columns\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    n_rows = matrix2.shape[0]\n",
    "    distance = np.zeros([n_rows])\n",
    "    for i in range(n_rows):\n",
    "        distance[i] = np.linalg.norm(target - matrix2[i,:])\n",
    "    return distance\n",
    "\n",
    "class Tracker:\n",
    "    def __init__(self,initial_position,kalman_filter:KalmanFilter,initial_covariance = 10):\n",
    "        self.target = initial_position\n",
    "        self.tracks = [initial_position.copy()]\n",
    "        self.kalman = kalman_filter\n",
    "        self.covariance_target = np.eye(8) * initial_covariance\n",
    "\n",
    "    def find_observation(self,new_detections, threshold=80):\n",
    "        ###### CODE HERE ######\n",
    "        pass\n",
    "\n",
    "\n",
    "        # return format_detections(new_detections[index_lower][None])\n",
    "\n",
    "    def process_frame(self,new_detections):\n",
    "\n",
    "        observation = self.find_observation(new_detections)\n",
    "\n",
    "        if len(new_detections) == 0:\n",
    "            u = np.zeros([4])\n",
    "        # else:\n",
    "        ###### CODE HERE ######\n",
    "\n",
    "        prior_location = self.kalman.prior(self.target,u)\n",
    "\n",
    "        if len(observation) > 0:\n",
    "\n",
    "            # observation = format_detections(observation[:4])\n",
    "            observation = observation[0,:]\n",
    "            posterior_location, updated_covariance =  self.kalman.posterior(x=self.target,\n",
    "                                                                                    prior_x = prior_location,\n",
    "                                                                                    u = np.zeros([4]),\n",
    "                                                                                    observation = observation,\n",
    "                                                                                    P = self.covariance_target)\n",
    "            self.covariance_target = updated_covariance\n",
    "            self.target = posterior_location\n",
    "            self.tracks.append(posterior_location.copy())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def plot_tracking_position(self,img,color=(0,0.6,0.05)):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        p1 = self.target[:2]\n",
    "        wh = self.target[2:4] - self.target[:2]\n",
    "        ax.add_patch(Rectangle(p1.tolist(), wh[0], wh[1],\n",
    "                               edgecolor = 'red',\n",
    "                               facecolor = 'blue',\n",
    "                               fill=False,\n",
    "                               lw=2))\n",
    "\n",
    "        variance_x = self.covariance_target[0,0]\n",
    "        variance_y = self.covariance_target[1,1]\n",
    "        range_variance1 = np.array([np.sqrt(variance_x),np.sqrt(variance_y)])\n",
    "        variance_x = self.covariance_target[2,2]\n",
    "        variance_y = self.covariance_target[3,3]\n",
    "        range_variance2 = np.array([np.sqrt(variance_x),np.sqrt(variance_y)])\n",
    "        p1 = self.target[:2] - range_variance1\n",
    "        p2 = self.target[2:4] + range_variance2\n",
    "        wh = p2 - p1\n",
    "\n",
    "\n",
    "        #create simple line plot\n",
    "        ax.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        #add rectangle to plot\n",
    "\n",
    "        ax.add_patch(Rectangle(p1.tolist(), wh[0], wh[1],\n",
    "                               edgecolor = color,\n",
    "                               fill=False,\n",
    "                               linestyle ='--',\n",
    "                               lw=2))\n",
    "tracker = Tracker(detections_poses[0,:],kalman)\n",
    "id_frame = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2cf949-3fd9-420a-b107-230f5f4ba803",
   "metadata": {
    "id": "5e2cf949-3fd9-420a-b107-230f5f4ba803"
   },
   "outputs": [],
   "source": [
    "id_frame += 1\n",
    "tracks,img,dets = dataset.get_frame_and_detections(id_frame,draw_detection=True)\n",
    "# plt.imshow(img)\n",
    "# plt.show()\n",
    "tracker.process_frame(dets)\n",
    "tracker.plot_tracking_position(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2V1-ov2hKOW3",
   "metadata": {
    "id": "2V1-ov2hKOW3"
   },
   "outputs": [],
   "source": [
    "tracker.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307f188c-d25a-44cf-b456-40f9b7b7cb41",
   "metadata": {
    "id": "307f188c-d25a-44cf-b456-40f9b7b7cb41"
   },
   "source": [
    "# Tracking cherries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6772e974-809f-4752-a972-3b05ddb59f87",
   "metadata": {
    "id": "6772e974-809f-4752-a972-3b05ddb59f87"
   },
   "outputs": [],
   "source": [
    "\n",
    "bytetrack_params = \"\"\"\n",
    "tracker_type: bytetrack # (str) Tracker backend: botsort|bytetrack; choose bytetrack for the classic baseline\n",
    "track_high_thresh: 0.25 # (float) First-stage match threshold; raise for cleaner tracks, lower to keep more\n",
    "track_low_thresh: 0.1 # (float) Second-stage threshold for low-score matches; balances recovery vs drift\n",
    "new_track_thresh: 0.25 # (float) Start a new track if no match ≥ this; higher reduces false tracks\n",
    "track_buffer: 30 # (int) Frames to keep lost tracks alive; higher handles occlusion, increases ID switches risk\n",
    "match_thresh: 0.8 # (float) Association similarity threshold (IoU/cost); tune with detector quality\n",
    "fuse_score: True # (bool) Fuse detection score with motion/IoU for matching; stabilizes weak detections\n",
    "\"\"\"\n",
    "\n",
    "botsort_params = \"\"\"\n",
    "tracker_type: botsort # (str) Tracker backend: botsort|bytetrack; choose botsort to enable BoT-SORT features\n",
    "track_high_thresh: 0.25 # (float) First-stage match threshold; raise for cleaner tracks, lower to keep more\n",
    "track_low_thresh: 0.1 # (float) Second-stage threshold for low-score matches; balances recovery vs drift\n",
    "new_track_thresh: 0.25 # (float) Start a new track if no match ≥ this; higher reduces false tracks\n",
    "track_buffer: 30 # (int) Frames to keep lost tracks alive; higher handles occlusion, increases ID switches risk\n",
    "match_thresh: 0.8 # (float) Association similarity threshold (IoU/cost); tune with detector quality\n",
    "fuse_score: True # (bool) Fuse detection score with motion/IoU for matching; stabilizes weak detections\n",
    "\n",
    "# BoT-SORT specifics\n",
    "gmc_method: sparseOptFlow # (str) Global motion compensation: sparseOptFlow|orb|none; helps moving camera scenes\n",
    "\n",
    "# ReID model related thresh\n",
    "proximity_thresh: 0.5 # (float) Min IoU to consider tracks proximate for ReID; higher is stricter\n",
    "appearance_thresh: 0.8 # (float) Min appearance similarity for ReID; raise to avoid identity swaps\n",
    "with_reid: False # (bool) Enable ReID model use; needs extra model and compute\n",
    "model: auto # (str) ReID model name/path; \"auto\" uses detector features if available\n",
    "\"\"\"\n",
    "\n",
    "bytetrack_dict = yaml.safe_load(bytetrack_params)\n",
    "botsort_dict = yaml.safe_load(botsort_params)\n",
    "with open('bytetrack.yaml', 'w') as file:\n",
    "    yaml.dump(bytetrack_dict, file)\n",
    "\n",
    "with open('botsort.yaml', 'w') as file:\n",
    "    yaml.dump(botsort_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b625c3b5-2aba-44ff-8b03-f79507dbd0f3",
   "metadata": {
    "id": "b625c3b5-2aba-44ff-8b03-f79507dbd0f3"
   },
   "source": [
    "# Video Tracking\n",
    "1. Generate a video with tracked cherries and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7fa66f-e0ba-477b-ae85-39845a58f6e4",
   "metadata": {
    "id": "8f7fa66f-e0ba-477b-ae85-39845a58f6e4"
   },
   "outputs": [],
   "source": [
    "model = YOLO('yolov11_best2.pt')\n",
    "path_video = './short_video2.mov'\n",
    "results = model.track(path_video, conf=0.3, iou=0.5, save=True, show=True, tracker=\"bytetrack.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_rfWOCFFoZej",
   "metadata": {
    "id": "_rfWOCFFoZej"
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fea5a7-484f-4197-b668-08ae288218c9",
   "metadata": {
    "id": "f1fea5a7-484f-4197-b668-08ae288218c9"
   },
   "source": [
    "# Count and classify\n",
    "1. Count how many cherries are across the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5d25c9-7e8d-4b98-9369-f924ab91da72",
   "metadata": {
    "id": "ea5d25c9-7e8d-4b98-9369-f924ab91da72"
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(path_video)\n",
    "success = True\n",
    "# Loop through the video frames\n",
    "while success:\n",
    "# Read a frame from the video\n",
    "  success, frame = cap.read()\n",
    "\n",
    "  if success:\n",
    "    results = model.track([frame[:shape[1],:shape[1]]], persist=True)\n",
    "    ### CODE HERE"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ultra",
   "language": "python",
   "name": "ultra"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
